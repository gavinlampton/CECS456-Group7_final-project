{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries, loading Dataset, and defining x/y."
      ],
      "metadata": {
        "id": "jW70h9DR6BD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "raw_df = pd.read_excel(\"./Real estate valuation data set.xlsx\")\n",
        "del raw_df['No']\n",
        "del raw_df['X1 transaction date']\n",
        "\n",
        "X=raw_df.iloc[:,:-1].values\n",
        "y=raw_df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "nU5yKceL6Avy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into Training and Test sets."
      ],
      "metadata": {
        "id": "BVzgLV74yxKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "7BNxnm5mywjq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a Standardized dataset and a Normalized dataset."
      ],
      "metadata": {
        "id": "p6Ub8-7BytVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare both of these against each other and the raw dataset as a baseline."
      ],
      "metadata": {
        "id": "FwJyfTV07hJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardized dataset:"
      ],
      "metadata": {
        "id": "NUc_BvbvocP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "je68g117X_rj"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std_sc=StandardScaler()\n",
        "\n",
        "std_X_train = std_sc.fit_transform(X_train)\n",
        "std_X_test = std_sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized dataset:"
      ],
      "metadata": {
        "id": "ynpbhsZ9ofEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "norm = Normalizer()\n",
        "\n",
        "norm_X_train = norm.fit_transform(X_train)\n",
        "norm_X_test = norm.transform(X_test)"
      ],
      "metadata": {
        "id": "uKBYePYmoRt2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing PCA."
      ],
      "metadata": {
        "id": "TByQh2GbyzzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Raw data\n",
        "raw_pca = PCA(n_components=2, random_state=0)\n",
        "raw_X_train = raw_pca.fit_transform(X_train)\n",
        "raw_X_test = raw_pca.transform(X_test)\n",
        "print('Variance with 2 columns:', sum(raw_pca.explained_variance_ratio_ * 100))\n",
        "\n",
        "#Standardized data\n",
        "std_pca = PCA(n_components=2)\n",
        "std_X_train = std_pca.fit_transform(std_X_train)\n",
        "std_X_test = std_pca.transform(std_X_test)\n",
        "print('Variance with 2 columns and Standard Scaler:', sum(std_pca.explained_variance_ratio_ * 100))\n",
        "\n",
        "#Normalized data\n",
        "norm_pca = PCA(n_components=2)\n",
        "norm_X_train = norm_pca.fit_transform(norm_X_train)\n",
        "norm_X_test = norm_pca.transform(norm_X_test)\n",
        "print('Variance with 2 columns and Normalized:', sum(norm_pca.explained_variance_ratio_ * 100))"
      ],
      "metadata": {
        "id": "fanXvmrY5IUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb52e4d-fcec-497c-f4a8-e9cc33996d17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance with 2 columns: 99.99966533440927\n",
            "Variance with 2 columns and Standard Scaler: 100.0\n",
            "Variance with 2 columns and Normalized: 100.00000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "  \n",
        "print('Order:')\n",
        "print('MAE: Lower Better')\n",
        "print('MAE: Lower Better')\n",
        "print('R2: Higher Better')\n",
        "print()\n",
        "print('Only 2 columns, raw data')\n",
        "pca_regression = LinearRegression()\n",
        "pca_regression.fit(raw_X_train, y_train)\n",
        "y_pred_pca = pca_regression.predict(raw_X_test)\n",
        "print(mean_absolute_error(y_test, y_pred_pca))\n",
        "print(mean_squared_error(y_test, y_pred_pca))\n",
        "print(r2_score(y_test, y_pred_pca))\n",
        "\n",
        "print('\\nOnly 2 columns, standard scaler')\n",
        "pca_regression = LinearRegression()\n",
        "pca_regression.fit(std_X_train, y_train)\n",
        "y_pred_pca = pca_regression.predict(std_X_test)\n",
        "print(mean_absolute_error(y_test, y_pred_pca))\n",
        "print(mean_squared_error(y_test, y_pred_pca))\n",
        "print(r2_score(y_test, y_pred_pca))\n",
        "\n",
        "print('\\nOnly 2 columns, normalized')\n",
        "pca_regression = LinearRegression()\n",
        "pca_regression.fit(norm_X_train, y_train)\n",
        "y_pred_pca = pca_regression.predict(norm_X_test)\n",
        "print(mean_absolute_error(y_test, y_pred_pca))\n",
        "print(mean_squared_error(y_test, y_pred_pca))\n",
        "print(r2_score(y_test, y_pred_pca))\n",
        "\n",
        "print()\n",
        "print('\\nAll 5 columns')\n",
        "full_regressor = LinearRegression()\n",
        "full_regressor.fit(X_train, y_train)\n",
        "y_pred = full_regressor.predict(X_test)\n",
        "print(mean_absolute_error(y_test, y_pred))\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "print(r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "g1acDypBm0yL",
        "outputId": "11be29fe-d049-4d2e-c6c7-7c6376261cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order:\n",
            "MAE: Lower Better\n",
            "MAE: Lower Better\n",
            "R2: Higher Better\n",
            "\n",
            "Only 2 columns, raw data\n",
            "6.682875169367948\n",
            "78.63145466226699\n",
            "0.54731202359297\n",
            "\n",
            "Only 2 columns, standard scaler\n",
            "6.351839427574513\n",
            "69.49283092992846\n",
            "0.5999238581610362\n",
            "\n",
            "Only 2 columns, normalized\n",
            "6.690121730778055\n",
            "71.57281297524209\n",
            "0.5879492245096531\n",
            "\n",
            "\n",
            "All 5 columns\n",
            "5.7459274754626275\n",
            "62.172235622415606\n",
            "0.6420691483643861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5WJaSURnlQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}