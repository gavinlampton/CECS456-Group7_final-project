{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries, loading Dataset, and defining x/y."
      ],
      "metadata": {
        "id": "jW70h9DR6BD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "raw_df = pd.read_excel(\"./Real estate valuation data set.xlsx\")\n",
        "del raw_df['No']\n",
        "del raw_df['X1 transaction date']\n",
        "\n",
        "X=raw_df.iloc[:,:-1].values\n",
        "y=raw_df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "nU5yKceL6Avy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into Training and Test sets."
      ],
      "metadata": {
        "id": "BVzgLV74yxKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "7BNxnm5mywjq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a Standardized dataset and a Normalized dataset."
      ],
      "metadata": {
        "id": "p6Ub8-7BytVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare both of these against each other and the raw dataset as a baseline."
      ],
      "metadata": {
        "id": "FwJyfTV07hJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardized dataset:"
      ],
      "metadata": {
        "id": "NUc_BvbvocP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "je68g117X_rj"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std_sc=StandardScaler()\n",
        "\n",
        "std_X_train = std_sc.fit_transform(X_train)\n",
        "std_X_test = std_sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized dataset:"
      ],
      "metadata": {
        "id": "ynpbhsZ9ofEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "norm = Normalizer()\n",
        "\n",
        "norm_X_train = norm.fit_transform(X_train)\n",
        "norm_X_test = norm.transform(X_test)"
      ],
      "metadata": {
        "id": "uKBYePYmoRt2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing PCA."
      ],
      "metadata": {
        "id": "TByQh2GbyzzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Raw data\n",
        "raw_pca = PCA(n_components=2)\n",
        "X_train = raw_pca.fit_transform(X_train)\n",
        "X_test = raw_pca.transform(X_test)\n",
        "\n",
        "#Standardized data\n",
        "std_pca = PCA(n_components=2)\n",
        "std_X_train = std_pca.fit_transform(std_X_train)\n",
        "std_X_test = std_pca.transform(std_X_test)\n",
        "\n",
        "#Normalized data\n",
        "norm_pca = PCA(n_components=2)\n",
        "norm_X_train = norm_pca.fit_transform(norm_X_train)\n",
        "norm_X_test = norm_pca.transform(norm_X_test)"
      ],
      "metadata": {
        "id": "fanXvmrY5IUu"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}